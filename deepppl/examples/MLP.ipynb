{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from pyro import distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data.dataloader as dataloader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import deepppl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Basic example showing the interface of *DeepPPL*\n",
    "This example uses a NN: `MLP` adding uncertainity to its `parameters`\n",
    "\n",
    "The `DeepPPL` model should be built with the `mlp` as keyword argument\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```stan\n",
    "data {\n",
    "    int batch_size;\n",
    "    int <lower=0, upper=1> imgs[28,28,batch_size]; \n",
    "    int <lower=0, upper=10>  labels[batch_size];\n",
    "}\n",
    "\n",
    "networks {\n",
    "    MLP mlp with parameters:\n",
    "        l1.weight;\n",
    "        l1.bias;\n",
    "        l2.weight;\n",
    "        l2.bias;\n",
    "}\n",
    "\n",
    "prior\n",
    "{\n",
    "    mlp.l1.weight ~  Normal(zeros(mlp.l1.weight$shape), ones(mlp.l1.weight$shape));\n",
    "    mlp.l1.bias ~ Normal(zeros(mlp.l1.bias$shape), ones(mlp.l1.bias$shape));\n",
    "    mlp.l2.weight ~ Normal(zeros(mlp.l2.weight$shape), ones(mlp.l2.weight$shape));\n",
    "    mlp.l2.bias ~  Normal(zeros(mlp.l2.bias$shape), ones(mlp.l2.bias$shape));\n",
    "}\n",
    "\n",
    "guide_parameters\n",
    "{\n",
    "    real l1wloc[mlp.l1.weight$shape];\n",
    "    real l1wscale[mlp.l1.weight$shape];\n",
    "    real l1bloc[mlp.l1.bias$shape];\n",
    "    real l1bscale[mlp.l1.bias$shape];\n",
    "    real l2wloc[mlp.l2.weight$shape];\n",
    "    real l2wscale[mlp.l2.weight$shape];\n",
    "    real l2bloc[mlp.l2.bias$shape];\n",
    "    real l2bscale[mlp.l2.bias$shape];\n",
    "}\n",
    "\n",
    "guide {\n",
    "    l1wloc = randn(l1wloc$shape);\n",
    "    l1wscale = randn(l1wscale$shape);\n",
    "    mlp.l1.weight ~  Normal(l1wloc, softplus(l1wscale));\n",
    "    l1bloc = randn(l1bloc$shape);\n",
    "    l1bscale = randn(l1bscale$shape);\n",
    "    mlp.l1.bias ~ Normal(l1bloc, softplus(l1bscale));\n",
    "    l2wloc = randn(l2wloc$shape);\n",
    "    l2wscale = randn(l2wscale$shape);\n",
    "    mlp.l2.weight ~ Normal(l2wloc, softplus(l2wscale));\n",
    "    l2bloc = randn(l2bloc$shape);\n",
    "    l2bscale = randn(l2bscale$shape);\n",
    "    mlp.l2.bias ~ Normal(l2bloc, softplus(l2bscale));\n",
    "}\n",
    "\n",
    "model {\n",
    "    real logits[batch_size];\n",
    "    logits = mlp(imgs);\n",
    "    labels ~ CategoricalLogits(logits);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, nx, nh, ny = 128, 28 * 28, 1024, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(nx, nh)\n",
    "        self.l2 = torch.nn.Linear(nh, ny)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.l1(x.view((-1, nx))))\n",
    "        yhat = self.l2(h)\n",
    "        return F.log_softmax(yhat, dim=-1)\n",
    "\n",
    "mlp = MLP()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(batch_size):\n",
    "    train = MNIST(os.environ.get(\"DATA_DIR\", '.') + \"/data\", train=True, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),  # ToTensor does min-max normalization.\n",
    "    ]), )\n",
    "    test = MNIST(os.environ.get(\"DATA_DIR\", '.') + \"/data\", train=False, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),  # ToTensor does min-max normalization.\n",
    "    ]), )\n",
    "    dataloader_args = dict(shuffle=True, batch_size=batch_size,\n",
    "                        num_workers=3, pin_memory=False)\n",
    "    train_loader = dataloader.DataLoader(train, **dataloader_args)\n",
    "    test_loader = dataloader.DataLoader(test, **dataloader_args)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategoricalLogits(logits):\n",
    "    return dist.Categorical(logits=logits)\n",
    "\n",
    "def predict(data, posterior):\n",
    "    predictions = [model(data) for model in posterior]\n",
    "    prediction = torch.stack(predictions).mean(dim=0)\n",
    "    return prediction.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build `DpPPPL` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = loadData(batch_size)\n",
    "model = deepppl.DppplModel(model_file = '../tests/good/mlp.stan', mlp=mlp, CategoricalLogits=CategoricalLogits)\n",
    "svi = model.svi(params = {'lr' : 0.01})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model using `svi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Iteration:99 Loss:11644.133906364441\n",
      "Epoch:0 Iteration:199 Loss:11076.26720237732\n",
      "Epoch:0 Iteration:299 Loss:13080.560114860535\n",
      "Epoch:0 Iteration:399 Loss:12650.329202651978\n",
      "Epoch:1 Iteration:99 Loss:11038.329931259155\n",
      "Epoch:1 Iteration:199 Loss:9502.619313240051\n",
      "Epoch:1 Iteration:299 Loss:13369.01606464386\n",
      "Epoch:1 Iteration:399 Loss:11262.966876983643\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    for j, (imgs, lbls) in enumerate(train_loader, 0):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss = svi.step(batch_size, imgs, lbls)\n",
    "        if (j+1) % 100 == 0:\n",
    "            print('Epoch:{} Iteration:{} Loss:{}'.format(epoch, j, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute a posterior distribution\n",
    "In this case, the distribution is a distribution over possible MLPs. \n",
    "Each MLP will give a prediction and the uncertainity can be seen in the distribution of the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = svi.posterior(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "For each element in the testset, we expect the accuracy to be higher than 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, data in enumerate(test_loader):\n",
    "    images, labels = data\n",
    "    accuracy = (predict(images, posterior) == labels).type(torch.float).mean()\n",
    "    assert accuracy > 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a single batch can be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  8,  1,  2,  9,  6,  9,  4,  6,  2,  9,  2,  5,  7,\n",
       "         5,  5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(images, posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
