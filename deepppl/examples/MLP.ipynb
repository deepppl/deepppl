{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from pyro import distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data.dataloader as dataloader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import deepppl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Basic example showing the interface of *DeepPPL*\n",
    "This example uses a NN: `MLP` adding uncertainity to its `parameters`\n",
    "\n",
    "The `DeepPPL` model should be built with the `mlp` as keyword argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "networks {\n",
      "    MLP mlp;\n",
      "}\n",
      "\n",
      "data {\n",
      "    int batch_size;\n",
      "    int <lower=0, upper=1> imgs[28,28,batch_size];\n",
      "    int <lower=0, upper=10>  labels[batch_size];\n",
      "}\n",
      "\n",
      "parameters {\n",
      "    real mlp.l1.weight[*];\n",
      "    real mlp.l1.bias[*];\n",
      "    real mlp.l2.weight[*];\n",
      "    real mlp.l2.bias[*];\n",
      "}\n",
      "\n",
      "model {\n",
      "    real logits[batch_size];\n",
      "    mlp.l1.weight ~  normal(zeros(mlp.l1.weight$shape), ones(mlp.l1.weight$shape));\n",
      "    mlp.l1.bias ~ normal(zeros(mlp.l1.bias$shape), ones(mlp.l1.bias$shape));\n",
      "    mlp.l2.weight ~ normal(zeros(mlp.l2.weight$shape), ones(mlp.l2.weight$shape));\n",
      "    mlp.l2.bias ~  normal(zeros(mlp.l2.bias$shape), ones(mlp.l2.bias$shape));\n",
      "\n",
      "    logits = mlp(imgs);\n",
      "    labels ~ categorical_logits(logits);\n",
      "}\n",
      "\n",
      "guide parameters {\n",
      "    real l1wloc[mlp.l1.weight$shape];\n",
      "    real l1wscale[mlp.l1.weight$shape];\n",
      "    real l1bloc[mlp.l1.bias$shape];\n",
      "    real l1bscale[mlp.l1.bias$shape];\n",
      "    real l2wloc[mlp.l2.weight$shape];\n",
      "    real l2wscale[mlp.l2.weight$shape];\n",
      "    real l2bloc[mlp.l2.bias$shape];\n",
      "    real l2bscale[mlp.l2.bias$shape];\n",
      "}\n",
      "\n",
      "guide {\n",
      "    l1wloc = randn();\n",
      "    l1wscale = randn();\n",
      "    mlp.l1.weight ~  normal(l1wloc, softplus(l1wscale));\n",
      "    l1bloc = randn();\n",
      "    l1bscale = randn();\n",
      "    mlp.l1.bias ~ normal(l1bloc, softplus(l1bscale));\n",
      "    l2wloc = randn();\n",
      "    l2wscale = randn();\n",
      "    mlp.l2.weight ~ normal(l2wloc, softplus(l2wscale));\n",
      "    l2bloc = randn();\n",
      "    l2bscale = randn();\n",
      "    mlp.l2.bias ~ normal(l2bloc, softplus(l2bscale));\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../tests/good/mlp.stan', 'r') as source:\n",
    "    print(source.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, nx, nh, ny = 128, 28 * 28, 1024, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(nx, nh)\n",
    "        self.l2 = torch.nn.Linear(nh, ny)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.l1(x.view((-1, nx))))\n",
    "        yhat = self.l2(h)\n",
    "        return F.log_softmax(yhat, dim=-1)\n",
    "\n",
    "mlp = MLP()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(batch_size):\n",
    "    train = MNIST(os.environ.get(\"DATA_DIR\", '.') + \"/data\", train=True, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),  # ToTensor does min-max normalization.\n",
    "    ]), )\n",
    "    test = MNIST(os.environ.get(\"DATA_DIR\", '.') + \"/data\", train=False, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),  # ToTensor does min-max normalization.\n",
    "    ]), )\n",
    "    dataloader_args = dict(shuffle=True, batch_size=batch_size,\n",
    "                        num_workers=3, pin_memory=False)\n",
    "    train_loader = dataloader.DataLoader(train, **dataloader_args)\n",
    "    test_loader = dataloader.DataLoader(test, **dataloader_args)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_logits(logits):\n",
    "    return dist.Categorical(logits=logits)\n",
    "\n",
    "def predict(data, posterior):\n",
    "    predictions = [model(data) for model in posterior]\n",
    "    prediction = torch.stack(predictions).mean(dim=0)\n",
    "    return prediction.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build `DeepPPL` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = loadData(batch_size)\n",
    "model = deepppl.PyroModel(model_file = '../tests/good/mlp.stan', mlp=mlp, categorical_logits=categorical_logits)\n",
    "svi = model.svi(params = {'lr' : 0.01})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model using `svi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Iteration:99 Loss:401147.25935173035\n",
      "Epoch:0 Iteration:199 Loss:228531.69188594818\n",
      "Epoch:0 Iteration:299 Loss:149949.97155189514\n",
      "Epoch:0 Iteration:399 Loss:101423.53450012207\n",
      "Epoch:1 Iteration:99 Loss:60797.30235862732\n",
      "Epoch:1 Iteration:199 Loss:48374.308643341064\n",
      "Epoch:1 Iteration:299 Loss:34786.00557613373\n",
      "Epoch:1 Iteration:399 Loss:29633.180724143982\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    for j, (imgs, lbls) in enumerate(train_loader, 0):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss = svi.step(batch_size, imgs, lbls)\n",
    "        if (j+1) % 100 == 0:\n",
    "            print('Epoch:{} Iteration:{} Loss:{}'.format(epoch, j, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute a posterior distribution\n",
    "In this case, the distribution is a distribution over possible MLPs. \n",
    "Each MLP will give a prediction and the uncertainity can be seen in the distribution of the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = svi.posterior(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "For each element in the testset, we expect the accuracy to be higher than 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, data in enumerate(test_loader):\n",
    "    images, labels = data\n",
    "    accuracy = (predict(images, posterior) == labels).type(torch.float).mean()\n",
    "    assert accuracy > 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a single batch can be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 1, 5, 2, 5, 9, 7, 4, 0, 8, 4, 4, 1, 4, 2],\n",
       "       grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(images, posterior)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
